{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK hackfest-in-a-box for TELCO - LAB 2\n",
    "\n",
    "The objetive is this lab is to perform model training with Spark MLLib Random Forest Classification model for the customer churn. Random forests are ensembles of decision trees. Random forests are one of the most successful machine learning models for classification and regression. They combine many decision trees in order to reduce the risk of overfitting. Like decision trees, random forests handle categorical features, extend to the multiclass classification setting, do not require feature scaling, and are able to capture non-linearities and feature interactions.\n",
    "\n",
    "`spark.mllib` supports random forests for binary and multiclass classification and for regression, using both continuous and categorical features. `spark.mllib` implements random forests using the existing decision tree implementation. Please see the decision tree guide for more information on `trees.prediction`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Initial data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import sys, logging, argparse, random, tempfile, json\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StructType, DoubleType, StringType\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-defined input variables\n",
    "print('....Setting input variables')\n",
    "projectNbr = \"YOUR_PROJECT_NBR\"\n",
    "projectID = \"YOUR_PROJECT_ID\"\n",
    "appBaseName = \"customer-churn-model\"\n",
    "appNameSuffix = \"training\"\n",
    "appName = f\"{appBaseName}-{appNameSuffix}\"\n",
    "modelBaseNm = appBaseName\n",
    "bqDatasetNm = f\"{projectID}.customer_churn_ds\"\n",
    "operation = appNameSuffix\n",
    "bigQuerySourceTableFQN = f\"{bqDatasetNm}.training_data_notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARK session creation\n",
    "print('....Initializing spark & spark configs')\n",
    "spark = SparkSession.builder.appName(appName).getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training data and typecasting\n",
    "print('....Read the training dataset into a dataframe')\n",
    "inputDF = spark.read \\\n",
    "    .format('bigquery') \\\n",
    "    .load(bigQuerySourceTableFQN)\n",
    "# Typecast some columns to the right datatype\n",
    "inputDF = inputDF.withColumn(\"monthly_charges\", inputDF.monthly_charges.cast('float')) \\\n",
    "    .withColumn(\"total_charges\", inputDF.total_charges.cast('float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Split the data in training and test datasets\n",
    "* Use `randomSplit` to split the data into the `trainDF`, `testDF` dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE\n",
    "SPLIT_SEED = 6\n",
    "SPLIT_SPECS = [0.8, 0.2]\n",
    "print('....Splitting the dataset')\n",
    "trainDF, testDF = inputDF.randomSplit(SPLIT_SPECS, seed=SPLIT_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering or feature extraction or feature discovery is the process of using domain knowledge to extract features (characteristics, properties, attributes) from raw data. The motivation is to use these extra features to improve the quality of results from a machine learning process, compared with supplying only the raw data to the machine learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - One hot encode the categorical columns\n",
    "* One hot encode  each text categorical column that you have identified in the previous lab using first `stringIndexer` and then `OneHotEncoderEstimator`\n",
    "* Do not fit the transformations yet, instead maintain a list with all the required steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE\n",
    "print('....Data pre-procesing: One hot encoding of categotical columns')\n",
    "CATEGORICAL_COLUMN_LIST = ['gender', 'senior_citizen', 'partner', 'dependents', 'phone_service', 'multiple_lines',\n",
    "                        'internet_service', 'online_security', 'online_backup', 'device_protection', 'tech_support',\n",
    "                        'streaming_tv', 'streaming_movies', 'contract', 'paperless_billing', 'payment_method']  \n",
    "dataPreprocessingStagesList = []\n",
    "for eachCategoricalColumn in CATEGORICAL_COLUMN_LIST:\n",
    "    stringIndexer = StringIndexer(inputCol=eachCategoricalColumn, outputCol=eachCategoricalColumn + \"Index\")\n",
    "    # https://spark.apache.org/docs/3.0.0-preview/ml-migration-guide.html \n",
    "    if (spark.version).startswith(\"2.\"):\n",
    "        from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[eachCategoricalColumn + \"classVec\"])\n",
    "    elif (spark.version).startswith(\"3.\"):\n",
    "        from pyspark.ml.feature import OneHotEncoder\n",
    "        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[eachCategoricalColumn + \"classVec\"])\n",
    "    else:\n",
    "        from pyspark.ml.feature import OneHotEncoder\n",
    "        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[eachCategoricalColumn + \"classVec\"])\n",
    "    dataPreprocessingStagesList += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Generate labels for the output variable (churn)\n",
    "* Use `stringIndexer` and generate labels for the churn column\n",
    "* Do not fit the transformations yet, instead append the transformation to the previous list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE\n",
    "print('....Data pre-procesing: Labels for target columns')\n",
    "labelStringIndexer = StringIndexer(inputCol=\"churn\", outputCol=\"label\")\n",
    "dataPreprocessingStagesList += [labelStringIndexer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Assemble all the features into the `features` columns\n",
    "* Use `VectorAssembler` and a `features` column with the one hot encoded features + previous numerical features\n",
    "* Do not fit the transformations yet, instead append the transformation to the previous list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE\n",
    "NUMERIC_COLUMN_LIST = ['monthly_charges', 'total_charges']\n",
    "print('....Generating features column')\n",
    "assemblerInputs = NUMERIC_COLUMN_LIST + [c + \"classVec\" for c in CATEGORICAL_COLUMN_LIST]\n",
    "featuresVectorAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "dataPreprocessingStagesList += [featuresVectorAssembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Model training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Generate a Random Forest Classifier training step and the training pipeline\n",
    "* Use `RandomForestClassifier` to define the training step\n",
    "* Do not fit the transformations yet, instead append the transformation to a new list\n",
    "* Generate a training pipeline using `Pipeline` putting together the feature engineering steps and the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE\n",
    "print('....Model training')\n",
    "modelTrainingStageList = []\n",
    "rfClassifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "modelTrainingStageList += [rfClassifier]\n",
    "print('....Generating the pipeline')\n",
    "pipeline = Pipeline(stages=dataPreprocessingStagesList + modelTrainingStageList) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Fit the training pipeline\n",
    "* Use `fit` to execute the training pipeline over the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE\n",
    "print('....Fit the model')\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Test the model with the test dataset\n",
    "* Use `transform` to execute predidctions using the fitted model over the `testDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE\n",
    "print('....Test the model')\n",
    "predictionsDF = pipelineModel.transform(testDF)\n",
    "predictionsDF.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Calculate the area under ROC to asses the quality of the classifier\n",
    "* Use `BinaryClassificationEvaluator` to calculathe the `areaUnderROC` metric\n",
    "* What do you think about the value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE\n",
    "print('....Calculating area under the ROC curve')\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setRawPredictionCol(\"prediction\")\n",
    "evaluator.setLabelCol(\"label\")\n",
    "value = evaluator.evaluate(predictionsDF, {evaluator.metricName: \"areaUnderROC\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsDF = spark.createDataFrame( [(\"areaUnderROC\",value)], [\"metric_nm\", \"metric_value\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of LAB 2**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "serverless_spark": "{\"name\":\"projects/s8s-spark-ml-mlops/locations/us-central1/sessions/agni-6\",\"uuid\":\"35fda7e3-be7b-4913-99c5-83e97b677386\",\"createTime\":\"2022-08-04T02:37:17.836903Z\",\"jupyterSession\":{},\"spark\":{},\"runtimeInfo\":{},\"state\":\"ACTIVE\",\"stateTime\":\"2022-08-04T02:38:37.084371Z\",\"creator\":\"s8s-lab-sa@s8s-spark-ml-mlops.iam.gserviceaccount.com\",\"runtimeConfig\":{\"containerImage\":\"gcr.io/s8s-spark-ml-mlops/dataproc_serverless_custom_runtime:1.0.3\",\"properties\":{\"spark:spark.executor.instances\":\"2\",\"spark:spark.driver.cores\":\"4\",\"spark:spark.executor.cores\":\"4\",\"spark:spark.eventLog.dir\":\"gs://s8s-sphs-974925525028/35fda7e3-be7b-4913-99c5-83e97b677386/spark-job-history\"}},\"environmentConfig\":{\"executionConfig\":{\"serviceAccount\":\"s8s-lab-sa@s8s-spark-ml-mlops.iam.gserviceaccount.com\",\"subnetworkUri\":\"https://www.googleapis.com/compute/v1/projects/s8s-spark-ml-mlops/regions/us-central1/subnetworks/spark-snet\"},\"peripheralsConfig\":{\"sparkHistoryServerConfig\":{\"dataprocCluster\":\"projects/s8s-spark-ml-mlops/regions/us-central1/clusters/s8s-sphs-974925525028\"}}},\"stateHistory\":[{\"state\":\"CREATING\",\"stateStartTime\":\"2022-08-04T02:37:17.836903Z\"}]}",
  "serverless_spark_kernel_name": "remote-bc514a4a91cec988ad3c15a7-pyspark",
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
