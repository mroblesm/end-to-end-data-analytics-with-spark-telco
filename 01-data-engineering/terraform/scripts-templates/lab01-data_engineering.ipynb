{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8023dc7c-ec1c-41a7-a049-c8e2a44a5beb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SPARK hackfest-in-a-box for TELCO - LAB 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objetive is this lab is to perform exploratory data analysis and data preparation on a customer churn telco dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, exploratory data analysis (EDA) is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-defined input variables\n",
    "print('....Setting input variables')\n",
    "projectNbr = \"YOUR_PROJECT_NBR\"\n",
    "projectID = \"YOUR_PROJECT_ID\"\n",
    "appBaseName = \"customer-churn-model\"\n",
    "appNameSuffix = \"preprocessing\"\n",
    "appName = f\"{appBaseName}-{appNameSuffix}\"\n",
    "sourceBucketUri = f\"gs://s8s_data_bucket-{projectNbr}/telco_customer_churn_train_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARK session creation\n",
    "print('....Initializing spark & spark configs')\n",
    "spark = SparkSession.builder.appName(appName).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "print('....Read source data')\n",
    "rawChurnDF = spark.read.options(inferSchema = True, header= True).csv(sourceBucketUri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table schema\n",
    "rawChurnDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - What do you think about the infered data types on the previous cell?\n",
    "* Show a couple of rows from the table using the `show` function\n",
    "* Count the number of rows using the `count` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawChurnDF._______INSERT_CODE_HERE_______(2,vertical=True)\n",
    "print(rawChurnDF._______INSERT_CODE_HERE_______)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Compute the per row distribution statistics, what business insights can you get?\n",
    "* Use the `describe` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawChurnDF._______INSERT_CODE_HERE_______.show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Clean the data, for each column, look for how many rows have None, NULLs, or `' '` values\n",
    "* Using `pyspark.sql.functions` \n",
    "* Using `spark.sql` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawChurnDF.select([count(_______INSERT_CODE_HERE_______, c )).alias(c) for c in rawChurnDF.columns]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE, use sql functions to find nulls, emptys, NULLS,NanS ..\n",
    "rawChurnDF.createOrReplaceTempView(\"base_customer_churn\")\n",
    "for c in rawChurnDF.columns:\n",
    "    print('Column: {}'.format(c))\n",
    "    spark.sql(_______INSERT_CODE_HERE_______).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Try to identify which columns have categorical values\n",
    "* Using `countDistinc` from `pyspark.sql.functions` \n",
    "* Using `spark.sql` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE\n",
    "rawChurnDF.select([_______INSERT_CODE_HERE_______ for c in rawChurnDF.columns]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Draw histograms to understand continous variables dsitributions, , what business insights can you get?\n",
    "* Transform results from `spark.sql` to pandas using `toPandas()` function \n",
    "* Draw histogra using `hist()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE\n",
    "MonthlyChargesPDF = spark.sql(_______INSERT_CODE_HERE_______).toPandas()\n",
    "MonthlyChargesPDF._______INSERT_CODE_HERE_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is the manipulation or dropping of data before it is used in order to ensure or enhance performance, and is an important step in the data mining process. The phrase \"garbage in, garbage out\" is particularly applicable to data mining and machine learning projects. Data-gathering methods are often loosely controlled, resulting in out-of-range values (e.g., Income: âˆ’100), impossible data combinations (e.g., Sex: Male, Pregnant: Yes), and missing values, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - List data preprocessing steps\n",
    "* Based on the insights derived from the previous section, this the proposed list the preprocessing steps you will apply to the dataset:\n",
    "    * Null,empty .. field replacement with `None` value\n",
    "    * Drop rows with `None` values\n",
    "    * Uniform values in rows (e.g. change No internet service or No phone service to No)\n",
    "    * Bucketize the `tenure` field\n",
    "    * Change field names to `snake_case` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Write a chain of data transformations serializing data at each step to ensure traceablity and debugging\n",
    "* Generate the following chain of datataframes `nullsReplacedDF`, `nullDroppedDF`,`partiallyProcessedDF`, `modelTrainingReadyDF` and `persistDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('....Replacing null, empty values ... with None')\n",
    "nullsReplacedDF=rawChurnDF.select([when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            (col(c) == ' ')  | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c),_______INSERT_CODE_HERE_______).otherwise(col(c)).alias(c) for c in rawChurnDF.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('....Number of rows before dropping None values')\n",
    "print(nullsReplacedDF._______INSERT_CODE_HERE_______)\n",
    "print('....Dropping None values')\n",
    "nullDroppedDF = nullsReplacedDF._______INSERT_CODE_HERE_______\n",
    "print('....Number of rows after dropping None values')\n",
    "print(nullDroppedDF._______INSERT_CODE_HERE_______)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('....Homogenization of categorical values')\n",
    "partiallyProcessedDF = nullDroppedDF.select(_______INSERT_CODE_HERE_______).otherwise(col(c)).alias(c) for c in nullDroppedDF.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('....Bucketizing the tenure field')\n",
    "partiallyProcessedDF.createOrReplaceTempView(\"partially_transformed_customer_churn\")\n",
    "modelTrainingReadyDF = spark.sql(\"\"\"\n",
    "                                select  customerID \n",
    "                                        ,gender as Gender\n",
    "                                        ,cast(SeniorCitizen as int) SeniorCitizen\n",
    "                                        ,Partner\n",
    "                                        ,Dependents\n",
    "                                        ,cast(tenure as int)  Tenure\n",
    "                                        ,case when _______INSERT_CODE_HERE_______ then \"Tenure_0-12\"\n",
    "                                              when _______INSERT_CODE_HERE_______ then \"Tenure_12-24\"\n",
    "                                              when _______INSERT_CODE_HERE_______ then \"Tenure_24-48\"\n",
    "                                              when _______INSERT_CODE_HERE_______ then \"Tenure_48-60\"\n",
    "                                              when _______INSERT_CODE_HERE_______ then \"Tenure_gt_60\"\n",
    "                                        end as Tenure_Group\n",
    "                                        ,PhoneService\n",
    "                                        ,MultipleLines\n",
    "                                        ,InternetService\n",
    "                                        ,OnlineSecurity\n",
    "                                        ,OnlineBackup\n",
    "                                        ,DeviceProtection\n",
    "                                        ,TechSupport\n",
    "                                        ,StreamingTV\n",
    "                                        ,StreamingMovies\n",
    "                                        ,Contract\n",
    "                                        ,PaperlessBilling\n",
    "                                        ,PaymentMethod\n",
    "                                        ,cast(MonthlyCharges as float) MonthlyCharges\n",
    "                                        ,cast(TotalCharges as float) TotalCharges\n",
    "                                        ,lcase(Churn) as Churn\n",
    "                                from partially_transformed_customer_churn  \n",
    "                                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('....Format column names in snake_case for consistency')\n",
    "persistDF = modelTrainingReadyDF.select(\"customerID\", \"gender\", \"SeniorCitizen\", \"Partner\", \"Dependents\", \"tenure\", \"Tenure_Group\", \"PhoneService\", \"MultipleLines\", \"InternetService\", \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\", \"MonthlyCharges\", \"TotalCharges\",\"Churn\") \\\n",
    "                                .toDF(\"customer_id\", \"gender\", \"senior_citizen\", \"partner\", \"dependents\", \"tenure\", \"tenure_group\", \"phone_service\", \"multiple_lines\", \"internet_service\", \"online_security\", \"online_backup\", \"device_protection\", \"tech_support\", \"streaming_tv\", \"streaming_movies\", \"contract\", \"paperless_billing\", \"payment_method\", \"monthly_charges\", \"total_charges\",\"churn\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('....Save data in BigQuery for next steps')\n",
    "bqDatasetNm = f\"{projectID}.customer_churn_ds\"\n",
    "bigQueryTargetTableFQN = f\"{bqDatasetNm}.training_data_notebook\"\n",
    "scratchBucketUri = f\"s8s-spark-bucket-{projectNbr}/{appBaseName}/{appNameSuffix}\"\n",
    "spark.conf.set(\"parentProject\", projectID)\n",
    "spark.conf.set(\"temporaryGcsBucket\", scratchBucketUri)\n",
    "\n",
    "persistDF.write.format('bigquery') \\\n",
    ".mode(\"append\")\\\n",
    ".option('table', bigQueryTargetTableFQN) \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Extra ball\n",
    "Once you have your data in BQ you can use this magic for a full fledge EdA\n",
    "* `%bigquery_stats spark-hackfest-dev.customer_churn_ds.training_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of LAB 1**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-data-engineering",
   "notebookOrigID": 1914343434663113,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "serverless_spark": "{\"name\":\"projects/s8s-spark-ml-mlops/locations/us-central1/sessions/agni-6\",\"uuid\":\"35fda7e3-be7b-4913-99c5-83e97b677386\",\"createTime\":\"2022-08-04T02:37:17.836903Z\",\"jupyterSession\":{},\"spark\":{},\"runtimeInfo\":{},\"state\":\"ACTIVE\",\"stateTime\":\"2022-08-04T02:38:37.084371Z\",\"creator\":\"s8s-lab-sa@s8s-spark-ml-mlops.iam.gserviceaccount.com\",\"runtimeConfig\":{\"containerImage\":\"gcr.io/s8s-spark-ml-mlops/dataproc_serverless_custom_runtime:1.0.3\",\"properties\":{\"spark:spark.executor.instances\":\"2\",\"spark:spark.driver.cores\":\"4\",\"spark:spark.executor.cores\":\"4\",\"spark:spark.eventLog.dir\":\"gs://s8s-sphs-974925525028/35fda7e3-be7b-4913-99c5-83e97b677386/spark-job-history\"}},\"environmentConfig\":{\"executionConfig\":{\"serviceAccount\":\"s8s-lab-sa@s8s-spark-ml-mlops.iam.gserviceaccount.com\",\"subnetworkUri\":\"https://www.googleapis.com/compute/v1/projects/s8s-spark-ml-mlops/regions/us-central1/subnetworks/spark-snet\"},\"peripheralsConfig\":{\"sparkHistoryServerConfig\":{\"dataprocCluster\":\"projects/s8s-spark-ml-mlops/regions/us-central1/clusters/s8s-sphs-974925525028\"}}},\"stateHistory\":[{\"state\":\"CREATING\",\"stateStartTime\":\"2022-08-04T02:37:17.836903Z\"}]}",
  "serverless_spark_kernel_name": "remote-bc514a4a91cec988ad3c15a7-pyspark",
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
