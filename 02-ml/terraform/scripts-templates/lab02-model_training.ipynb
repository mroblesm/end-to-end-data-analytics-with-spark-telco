{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK hackfest-in-a-box for TELCO - LAB 2\n",
    "\n",
    "The objetive is this lab is to perform model training with Spark MLLib Random Forest Classification model for the customer churn. Random forests are ensembles of decision trees. Random forests are one of the most successful machine learning models for classification and regression. They combine many decision trees in order to reduce the risk of overfitting. Like decision trees, random forests handle categorical features, extend to the multiclass classification setting, do not require feature scaling, and are able to capture non-linearities and feature interactions.\n",
    "\n",
    "`spark.mllib` supports random forests for binary and multiclass classification and for regression, using both continuous and categorical features. `spark.mllib` implements random forests using the existing decision tree implementation. Please see the decision tree guide for more information on `trees.prediction`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Initial data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import sys, logging, argparse, random, tempfile, json\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StructType, DoubleType, StringType\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Setting input variables\n"
     ]
    }
   ],
   "source": [
    "print('....Setting input variables')\n",
    "projectNbr = \"YOUR_PROJECT_NBR\"\n",
    "projectID = \"YOUR_PROJECT_ID\"\n",
    "appBaseName = \"customer-churn-model\"\n",
    "appNameSuffix = \"training\"\n",
    "appName = f\"{appBaseName}-{appNameSuffix}\"\n",
    "modelBaseNm = appBaseName\n",
    "bqDatasetNm = f\"{projectID}.customer_churn_ds\"\n",
    "operation = appNameSuffix\n",
    "bigQuerySourceTableFQN = f\"{bqDatasetNm}.training_data_step_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Initializing spark & spark configs\n"
     ]
    }
   ],
   "source": [
    "print('....Initializing spark & spark configs')\n",
    "spark = SparkSession.builder.appName(appName).getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Read the training dataset into a dataframe\n"
     ]
    }
   ],
   "source": [
    "print('....Read the training dataset into a dataframe')\n",
    "inputDF = spark.read \\\n",
    "    .format('bigquery') \\\n",
    "    .load(bigQuerySourceTableFQN)\n",
    "# Typecast some columns to the right datatype\n",
    "inputDF = inputDF.withColumn(\"monthly_charges\", inputDF.monthly_charges.cast('float')) \\\n",
    "    .withColumn(\"total_charges\", inputDF.total_charges.cast('float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Split the data in training and test datasets\n",
    "* Use `randomSplit` to split the data into the `trainDF`, `testDF` dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Splitting the dataset\n"
     ]
    }
   ],
   "source": [
    "SPLIT_SEED = 6\n",
    "SPLIT_SPECS = [0.8, 0.2]\n",
    "print('....Splitting the dataset')\n",
    "trainDF, testDF = inputDF._______INSERT_CODE_HERE_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering or feature extraction or feature discovery is the process of using domain knowledge to extract features (characteristics, properties, attributes) from raw data. The motivation is to use these extra features to improve the quality of results from a machine learning process, compared with supplying only the raw data to the machine learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - One hot encode the categorical columns\n",
    "* One hot encode  each text categorical column that you have identified in the previous lab using first `stringIndexer` and then `OneHotEncoderEstimator`\n",
    "* Do not fit the transformations yet, instead maintain a list with all the required steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Data pre-procesing: One hot encoding of categotical columns\n"
     ]
    }
   ],
   "source": [
    "print('....Data pre-procesing: One hot encoding of categotical columns')\n",
    "CATEGORICAL_COLUMN_LIST = ['gender', 'senior_citizen', 'partner', 'dependents', 'phone_service', 'multiple_lines',\n",
    "                        'internet_service', 'online_security', 'online_backup', 'device_protection', 'tech_support',\n",
    "                        'streaming_tv', 'streaming_movies', 'contract', 'paperless_billing', 'payment_method']  \n",
    "dataPreprocessingStagesList = []\n",
    "for eachCategoricalColumn in CATEGORICAL_COLUMN_LIST:\n",
    "    stringIndexer = StringIndexer(_______INSERT_CODE_HERE_______)\n",
    "    # https://spark.apache.org/docs/3.0.0-preview/ml-migration-guide.html \n",
    "    if (spark.version).startswith(\"2.\"):\n",
    "        from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "        encoder = OneHotEncoderEstimator(_______INSERT_CODE_HERE_______)\n",
    "    elif (spark.version).startswith(\"3.\"):\n",
    "        from pyspark.ml.feature import OneHotEncoder\n",
    "        encoder = OneHotEncoder(_______INSERT_CODE_HERE_______)\n",
    "    else:\n",
    "        from pyspark.ml.feature import OneHotEncoder\n",
    "        encoder = OneHotEncoder(_______INSERT_CODE_HERE_______)\n",
    "    dataPreprocessingStagesList += _______INSERT_CODE_HERE_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Generate labels for the output variable (churn)\n",
    "* Use `stringIndexer` and generate labels for the churn column\n",
    "* Do not fit the transformations yet, instead append the transformation to the previous list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Data pre-procesing: Labels for target columns\n"
     ]
    }
   ],
   "source": [
    "print('....Data pre-procesing: Labels for target columns')\n",
    "labelStringIndexer = StringIndexer(_______INSERT_CODE_HERE_______)\n",
    "dataPreprocessingStagesList += _______INSERT_CODE_HERE_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Assemble all the features into the `features` columns\n",
    "* Use `VectorAssembler` and a `features` column with the one hot encoded features + previous numerical features\n",
    "* Do not fit the transformations yet, instead append the transformation to the previous list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Generating features column\n"
     ]
    }
   ],
   "source": [
    "NUMERIC_COLUMN_LIST = ['monthly_charges', 'total_charges']\n",
    "print('....Generating features column')\n",
    "assemblerInputs = NUMERIC_COLUMN_LIST + [c + \"classVec\" for c in CATEGORICAL_COLUMN_LIST]\n",
    "featuresVectorAssembler = VectorAssembler(_______INSERT_CODE_HERE_______)\n",
    "dataPreprocessingStagesList += _______INSERT_CODE_HERE_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Model training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Generate a Random Forest Classifier training step and the training pipeline\n",
    "* Use `RandomForestClassifier` to define the training step\n",
    "* Do not fit the transformations yet, instead append the transformation to a new list\n",
    "* Generate a training pipeline using `Pipeline` putting together the feature engineering steps and the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Model training\n",
      "....Generating the pipeline\n"
     ]
    }
   ],
   "source": [
    "print('....Model training')\n",
    "modelTrainingStageList = []\n",
    "rfClassifier = RandomForestClassifier(_______INSERT_CODE_HERE_______)\n",
    "modelTrainingStageList += [rfClassifier]\n",
    "print('....Generating the pipeline')\n",
    "pipeline = Pipeline(_______INSERT_CODE_HERE_______) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Fit the training pipeline\n",
    "* Use `fit` to execute the training pipeline over the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Fit the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('....Fit the model')\n",
    "pipelineModel = pipeline._______INSERT_CODE_HERE_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Test the model with the test dataset\n",
    "* Use `transform` to execute predidctions using the fitted model over the `testDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Test the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/23 07:55:57 WARN org.apache.spark.util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------\n",
      " customer_id               | 0013-EXCHZ           \n",
      " gender                    | Female               \n",
      " senior_citizen            | 1                    \n",
      " partner                   | Yes                  \n",
      " dependents                | No                   \n",
      " tenure                    | 3                    \n",
      " tenure_group              | Tenure_0-12          \n",
      " phone_service             | Yes                  \n",
      " multiple_lines            | No                   \n",
      " internet_service          | Fiber optic          \n",
      " online_security           | No                   \n",
      " online_backup             | No                   \n",
      " device_protection         | No                   \n",
      " tech_support              | Yes                  \n",
      " streaming_tv              | Yes                  \n",
      " streaming_movies          | No                   \n",
      " contract                  | Month-to-month       \n",
      " paperless_billing         | Yes                  \n",
      " payment_method            | Mailed check         \n",
      " monthly_charges           | 83.9                 \n",
      " total_charges             | 267.4                \n",
      " churn                     | yes                  \n",
      " genderIndex               | 0.0                  \n",
      " genderclassVec            | (1,[0],[1.0])        \n",
      " senior_citizenIndex       | 1.0                  \n",
      " senior_citizenclassVec    | (1,[],[])            \n",
      " partnerIndex              | 1.0                  \n",
      " partnerclassVec           | (1,[],[])            \n",
      " dependentsIndex           | 0.0                  \n",
      " dependentsclassVec        | (1,[0],[1.0])        \n",
      " phone_serviceIndex        | 0.0                  \n",
      " phone_serviceclassVec     | (1,[0],[1.0])        \n",
      " multiple_linesIndex       | 0.0                  \n",
      " multiple_linesclassVec    | (1,[0],[1.0])        \n",
      " internet_serviceIndex     | 0.0                  \n",
      " internet_serviceclassVec  | (2,[0],[1.0])        \n",
      " online_securityIndex      | 0.0                  \n",
      " online_securityclassVec   | (1,[0],[1.0])        \n",
      " online_backupIndex        | 0.0                  \n",
      " online_backupclassVec     | (1,[0],[1.0])        \n",
      " device_protectionIndex    | 0.0                  \n",
      " device_protectionclassVec | (1,[0],[1.0])        \n",
      " tech_supportIndex         | 1.0                  \n",
      " tech_supportclassVec      | (1,[],[])            \n",
      " streaming_tvIndex         | 1.0                  \n",
      " streaming_tvclassVec      | (1,[],[])            \n",
      " streaming_moviesIndex     | 0.0                  \n",
      " streaming_moviesclassVec  | (1,[0],[1.0])        \n",
      " contractIndex             | 0.0                  \n",
      " contractclassVec          | (2,[0],[1.0])        \n",
      " paperless_billingIndex    | 0.0                  \n",
      " paperless_billingclassVec | (1,[0],[1.0])        \n",
      " payment_methodIndex       | 1.0                  \n",
      " payment_methodclassVec    | (3,[1],[1.0])        \n",
      " label                     | 1.0                  \n",
      " features                  | [83.9000015258789... \n",
      " rawPrediction             | [9.86422619547663... \n",
      " probability               | [0.49321130977383... \n",
      " prediction                | 1.0                  \n",
      "-RECORD 1-----------------------------------------\n",
      " customer_id               | 0013-SMEOE           \n",
      " gender                    | Female               \n",
      " senior_citizen            | 1                    \n",
      " partner                   | Yes                  \n",
      " dependents                | No                   \n",
      " tenure                    | 71                   \n",
      " tenure_group              | Tenure_gt_60         \n",
      " phone_service             | Yes                  \n",
      " multiple_lines            | No                   \n",
      " internet_service          | Fiber optic          \n",
      " online_security           | Yes                  \n",
      " online_backup             | Yes                  \n",
      " device_protection         | Yes                  \n",
      " tech_support              | Yes                  \n",
      " streaming_tv              | Yes                  \n",
      " streaming_movies          | Yes                  \n",
      " contract                  | Two year             \n",
      " paperless_billing         | Yes                  \n",
      " payment_method            | Bank transfer (au... \n",
      " monthly_charges           | 109.7                \n",
      " total_charges             | 7904.25              \n",
      " churn                     | no                   \n",
      " genderIndex               | 0.0                  \n",
      " genderclassVec            | (1,[0],[1.0])        \n",
      " senior_citizenIndex       | 1.0                  \n",
      " senior_citizenclassVec    | (1,[],[])            \n",
      " partnerIndex              | 1.0                  \n",
      " partnerclassVec           | (1,[],[])            \n",
      " dependentsIndex           | 0.0                  \n",
      " dependentsclassVec        | (1,[0],[1.0])        \n",
      " phone_serviceIndex        | 0.0                  \n",
      " phone_serviceclassVec     | (1,[0],[1.0])        \n",
      " multiple_linesIndex       | 0.0                  \n",
      " multiple_linesclassVec    | (1,[0],[1.0])        \n",
      " internet_serviceIndex     | 0.0                  \n",
      " internet_serviceclassVec  | (2,[0],[1.0])        \n",
      " online_securityIndex      | 1.0                  \n",
      " online_securityclassVec   | (1,[],[])            \n",
      " online_backupIndex        | 1.0                  \n",
      " online_backupclassVec     | (1,[],[])            \n",
      " device_protectionIndex    | 1.0                  \n",
      " device_protectionclassVec | (1,[],[])            \n",
      " tech_supportIndex         | 1.0                  \n",
      " tech_supportclassVec      | (1,[],[])            \n",
      " streaming_tvIndex         | 1.0                  \n",
      " streaming_tvclassVec      | (1,[],[])            \n",
      " streaming_moviesIndex     | 1.0                  \n",
      " streaming_moviesclassVec  | (1,[],[])            \n",
      " contractIndex             | 1.0                  \n",
      " contractclassVec          | (2,[1],[1.0])        \n",
      " paperless_billingIndex    | 0.0                  \n",
      " paperless_billingclassVec | (1,[0],[1.0])        \n",
      " payment_methodIndex       | 2.0                  \n",
      " payment_methodclassVec    | (3,[2],[1.0])        \n",
      " label                     | 0.0                  \n",
      " features                  | (22,[0,1,2,5,6,7,... \n",
      " rawPrediction             | [18.0166760513221... \n",
      " probability               | [0.90083380256610... \n",
      " prediction                | 0.0                  \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('....Test the model')\n",
    "predictionsDF = pipelineModel._______INSERT_CODE_HERE_______\n",
    "predictionsDF.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Calculate the area under ROC to asses the quality of the classifier\n",
    "* Use `BinaryClassificationEvaluator` to calculathe the `areaUnderROC` metric\n",
    "* What do you think about the value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Calculating area under the ROC curve\n"
     ]
    }
   ],
   "source": [
    "print('....Calculating area under the ROC curve')\n",
    "evaluator = _______INSERT_CODE_HERE_______\n",
    "evaluator.setRawPredictionCol(\"prediction\")\n",
    "evaluator.setLabelCol(\"label\")\n",
    "value = evaluator.evaluate(_______INSERT_CODE_HERE_______)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsDF = spark.createDataFrame( [(\"areaUnderROC\",value)], [\"metric_nm\", \"metric_value\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|   metric_nm|      metric_value|\n",
      "+------------+------------------+\n",
      "|areaUnderROC|0.6376611418047882|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metricsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of LAB 2**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "serverless_spark": "{\"name\":\"projects/s8s-spark-ml-mlops/locations/us-central1/sessions/agni-6\",\"uuid\":\"35fda7e3-be7b-4913-99c5-83e97b677386\",\"createTime\":\"2022-08-04T02:37:17.836903Z\",\"jupyterSession\":{},\"spark\":{},\"runtimeInfo\":{},\"state\":\"ACTIVE\",\"stateTime\":\"2022-08-04T02:38:37.084371Z\",\"creator\":\"s8s-lab-sa@s8s-spark-ml-mlops.iam.gserviceaccount.com\",\"runtimeConfig\":{\"containerImage\":\"gcr.io/s8s-spark-ml-mlops/dataproc_serverless_custom_runtime:1.0.3\",\"properties\":{\"spark:spark.executor.instances\":\"2\",\"spark:spark.driver.cores\":\"4\",\"spark:spark.executor.cores\":\"4\",\"spark:spark.eventLog.dir\":\"gs://s8s-sphs-974925525028/35fda7e3-be7b-4913-99c5-83e97b677386/spark-job-history\"}},\"environmentConfig\":{\"executionConfig\":{\"serviceAccount\":\"s8s-lab-sa@s8s-spark-ml-mlops.iam.gserviceaccount.com\",\"subnetworkUri\":\"https://www.googleapis.com/compute/v1/projects/s8s-spark-ml-mlops/regions/us-central1/subnetworks/spark-snet\"},\"peripheralsConfig\":{\"sparkHistoryServerConfig\":{\"dataprocCluster\":\"projects/s8s-spark-ml-mlops/regions/us-central1/clusters/s8s-sphs-974925525028\"}}},\"stateHistory\":[{\"state\":\"CREATING\",\"stateStartTime\":\"2022-08-04T02:37:17.836903Z\"}]}",
  "serverless_spark_kernel_name": "remote-bc514a4a91cec988ad3c15a7-pyspark",
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
